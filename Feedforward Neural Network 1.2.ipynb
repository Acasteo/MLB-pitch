{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"xor\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and merge dataset on ab_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").csv('pitches_preprocessed.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|latent_next_pitch|latent_pitch_type|\n",
      "+-----------------+-----------------+\n",
      "|              0.0|              0.0|\n",
      "|              0.0|              0.0|\n",
      "|              7.0|              0.0|\n",
      "|              7.0|              7.0|\n",
      "|              0.0|              7.0|\n",
      "|              6.0|              0.0|\n",
      "|              6.0|              6.0|\n",
      "|              7.0|              2.0|\n",
      "|              7.0|              7.0|\n",
      "|              3.0|              7.0|\n",
      "|              0.0|              2.0|\n",
      "|              2.0|              0.0|\n",
      "|              6.0|              3.0|\n",
      "|              6.0|              6.0|\n",
      "|              6.0|              6.0|\n",
      "|              6.0|              6.0|\n",
      "|              0.0|              6.0|\n",
      "|              2.0|              7.0|\n",
      "|              0.0|              3.0|\n",
      "|              0.0|              0.0|\n",
      "+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('latent_next_pitch', 'latent_pitch_type').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('outs', 'double'),\n",
       " ('pfx_x', 'double'),\n",
       " ('pfx_z', 'double'),\n",
       " ('pitch_num', 'double'),\n",
       " ('px', 'double'),\n",
       " ('pz', 'double'),\n",
       " ('start_speed', 'double'),\n",
       " ('sz_bot', 'double'),\n",
       " ('sz_top', 'double'),\n",
       " ('x0', 'double'),\n",
       " ('y0', 'double'),\n",
       " ('z0', 'double'),\n",
       " ('batter_id', 'int'),\n",
       " ('inning', 'int'),\n",
       " ('p_throws', 'int'),\n",
       " ('pitcher_id', 'int'),\n",
       " ('stand', 'int'),\n",
       " ('top', 'int'),\n",
       " ('score_difference', 'double'),\n",
       " ('latent_pitch_type', 'double'),\n",
       " ('latent_next_pitch', 'double'),\n",
       " ('count_status', 'int'),\n",
       " ('base_status', 'int'),\n",
       " ('binned_score_difference', 'int')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 3, -5, 5, 4, -4, -2, 2, -3, 0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('binned_score_difference').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(subset=[\"count_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+\n",
      "|binned_score_difference| count|\n",
      "+-----------------------+------+\n",
      "|                     -1|200888|\n",
      "|                      1|204003|\n",
      "|                      3| 95738|\n",
      "|                     -5|115029|\n",
      "|                      5|120903|\n",
      "|                      4| 66887|\n",
      "|                     -4| 63171|\n",
      "|                     -2|137364|\n",
      "|                      2|142395|\n",
      "|                     -3| 90426|\n",
      "|                      0|450747|\n",
      "+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('binned_score_difference').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|latent_pitch_type| count|\n",
      "+-----------------+------+\n",
      "|              8.0| 24866|\n",
      "|              0.0|611638|\n",
      "|              7.0| 40498|\n",
      "|              1.0|265031|\n",
      "|              4.0|143130|\n",
      "|              3.0|169526|\n",
      "|              2.0|195344|\n",
      "|             10.0|   916|\n",
      "|              6.0| 87888|\n",
      "|              5.0|142323|\n",
      "|              9.0|  6391|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('latent_pitch_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|latent_next_pitch| count|\n",
      "+-----------------+------+\n",
      "|              8.0| 29491|\n",
      "|              0.0|588341|\n",
      "|              7.0| 39122|\n",
      "|              1.0|279071|\n",
      "|              4.0|136854|\n",
      "|              3.0|190917|\n",
      "|              2.0|191363|\n",
      "|             10.0|   854|\n",
      "|              6.0| 90903|\n",
      "|              5.0|134291|\n",
      "|              9.0|  6344|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('latent_next_pitch').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|count_status| count|\n",
      "+------------+------+\n",
      "|           1|191344|\n",
      "|           6| 15064|\n",
      "|           3|184912|\n",
      "|           5| 93422|\n",
      "|           9| 24083|\n",
      "|           4| 63691|\n",
      "|           8|127543|\n",
      "|           7| 91912|\n",
      "|          10|100675|\n",
      "|          11| 31991|\n",
      "|           2|240205|\n",
      "|           0|522709|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('count_status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'outs',\n",
       " 'pfx_x',\n",
       " 'pfx_z',\n",
       " 'pitch_num',\n",
       " 'px',\n",
       " 'pz',\n",
       " 'start_speed',\n",
       " 'sz_bot',\n",
       " 'sz_top',\n",
       " 'x0',\n",
       " 'y0',\n",
       " 'z0',\n",
       " 'batter_id',\n",
       " 'inning',\n",
       " 'p_throws',\n",
       " 'pitcher_id',\n",
       " 'stand',\n",
       " 'top',\n",
       " 'score_difference',\n",
       " 'latent_pitch_type',\n",
       " 'latent_next_pitch',\n",
       " 'count_status',\n",
       " 'base_status',\n",
       " 'binned_score_difference']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('outs', 'pfx_x', 'pfx_z', 'pitch_num', 'px', 'pz', 'start_speed', 'sz_bot', 'sz_top', 'x0', 'y0',\n",
    " 'z0', 'batter_id', 'inning', 'p_throws', 'pitcher_id', 'stand', 'latent_pitch_type',\n",
    " 'count_status','base_status', 'binned_score_difference','latent_next_pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outs',\n",
       " 'pfx_x',\n",
       " 'pfx_z',\n",
       " 'pitch_num',\n",
       " 'px',\n",
       " 'pz',\n",
       " 'start_speed',\n",
       " 'sz_bot',\n",
       " 'sz_top',\n",
       " 'x0',\n",
       " 'y0',\n",
       " 'z0',\n",
       " 'batter_id',\n",
       " 'inning',\n",
       " 'p_throws',\n",
       " 'pitcher_id',\n",
       " 'stand',\n",
       " 'latent_pitch_type',\n",
       " 'count_status',\n",
       " 'base_status',\n",
       " 'binned_score_difference',\n",
       " 'latent_next_pitch']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('binned_score_difference', df.binned_score_difference +5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols =[\"latent_pitch_type\", \"pitch_num\", \"base_status\",\"binned_score_difference\",\n",
    "                                            \"count_status\"],\n",
    "                                 outputCols =[\"latent_pitch_typeH\", \"pitch_numH\", \"base_statusH\",\"binned_score_differenceH\"\n",
    "                                              ,\"count_statusH\"])\n",
    "model = encoder.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(latent_pitch_typeH=SparseVector(9, {0: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(9, {0: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(9, {0: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(9, {7: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(9, {7: 1.0}))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('latent_pitch_typeH').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outs',\n",
       " 'pfx_x',\n",
       " 'pfx_z',\n",
       " 'pitch_num',\n",
       " 'px',\n",
       " 'pz',\n",
       " 'start_speed',\n",
       " 'sz_bot',\n",
       " 'sz_top',\n",
       " 'x0',\n",
       " 'y0',\n",
       " 'z0',\n",
       " 'batter_id',\n",
       " 'inning',\n",
       " 'p_throws',\n",
       " 'pitcher_id',\n",
       " 'stand',\n",
       " 'latent_pitch_type',\n",
       " 'count_status',\n",
       " 'base_status',\n",
       " 'binned_score_difference',\n",
       " 'latent_next_pitch',\n",
       " 'count_statusH',\n",
       " 'base_statusH',\n",
       " 'binned_score_differenceH',\n",
       " 'latent_pitch_typeH',\n",
       " 'pitch_numH']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('outs','pfx_x','pfx_z','px','pz','start_speed','sz_bot','sz_top','x0',\n",
    "               'y0','z0','batter_id','inning','p_throws','pitcher_id','stand','latent_pitch_typeH','pitch_numH',\n",
    "               'base_statusH','binned_score_differenceH','count_statusH','latent_next_pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(latent_pitch_typeH=SparseVector(10, {0: 1.0}), pitch_numH=SparseVector(14, {1: 1.0}), base_statusH=SparseVector(7, {0: 1.0}), binned_score_differenceH=SparseVector(10, {4: 1.0}), count_statusH=SparseVector(11, {0: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(10, {0: 1.0}), pitch_numH=SparseVector(14, {2: 1.0}), base_statusH=SparseVector(7, {0: 1.0}), binned_score_differenceH=SparseVector(10, {4: 1.0}), count_statusH=SparseVector(11, {2: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(10, {0: 1.0}), pitch_numH=SparseVector(14, {1: 1.0}), base_statusH=SparseVector(7, {0: 1.0}), binned_score_differenceH=SparseVector(10, {}), count_statusH=SparseVector(11, {0: 1.0})),\n",
       " Row(latent_pitch_typeH=SparseVector(10, {7: 1.0}), pitch_numH=SparseVector(14, {2: 1.0}), base_statusH=SparseVector(7, {0: 1.0}), binned_score_differenceH=SparseVector(10, {}), count_statusH=SparseVector(11, {2: 1.0}))]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('latent_pitch_typeH','pitch_numH','base_statusH','binned_score_differenceH','count_statusH').take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------+------------------------+-------------+--------------+------------------+\n",
      "|label|            features|  count_statusH|binned_score_differenceH| base_statusH|    pitch_numH|latent_pitch_typeH|\n",
      "+-----+--------------------+---------------+------------------------+-------------+--------------+------------------+\n",
      "|  0.0|[1.0,6.08,9.83,-0...| (11,[0],[1.0])|          (10,[4],[1.0])|(7,[0],[1.0])|(14,[1],[1.0])|     (9,[0],[1.0])|\n",
      "|  0.0|[1.0,4.54,12.83,-...| (11,[2],[1.0])|          (10,[4],[1.0])|(7,[0],[1.0])|(14,[2],[1.0])|     (9,[0],[1.0])|\n",
      "|  7.0|[0.0,-3.71,9.05,-...| (11,[0],[1.0])|              (10,[],[])|(7,[0],[1.0])|(14,[1],[1.0])|     (9,[0],[1.0])|\n",
      "|  7.0|[0.0,4.87,-6.37,0...| (11,[2],[1.0])|              (10,[],[])|(7,[0],[1.0])|(14,[2],[1.0])|     (9,[7],[1.0])|\n",
      "|  0.0|[0.0,1.64,-4.12,0...| (11,[3],[1.0])|              (10,[],[])|(7,[0],[1.0])|(14,[3],[1.0])|     (9,[7],[1.0])|\n",
      "|  6.0|[0.0,-2.47,9.54,-...| (11,[8],[1.0])|              (10,[],[])|(7,[0],[1.0])|(14,[4],[1.0])|     (9,[0],[1.0])|\n",
      "|  6.0|[0.0,1.98,6.25,0....|(11,[10],[1.0])|              (10,[],[])|(7,[0],[1.0])|(14,[5],[1.0])|     (9,[6],[1.0])|\n",
      "|  7.0|[2.0,-6.24,7.53,-...| (11,[0],[1.0])|          (10,[6],[1.0])|(7,[0],[1.0])|(14,[1],[1.0])|     (9,[2],[1.0])|\n",
      "|  7.0|[2.0,2.25,-7.86,-...| (11,[1],[1.0])|          (10,[6],[1.0])|(7,[0],[1.0])|(14,[2],[1.0])|     (9,[7],[1.0])|\n",
      "|  3.0|[2.0,-0.03,-2.33,...| (11,[3],[1.0])|          (10,[6],[1.0])|(7,[0],[1.0])|(14,[3],[1.0])|     (9,[7],[1.0])|\n",
      "|  0.0|[2.0,-7.05,6.16,-...| (11,[7],[1.0])|          (10,[6],[1.0])|(7,[0],[1.0])|(14,[4],[1.0])|     (9,[2],[1.0])|\n",
      "|  2.0|[2.0,-3.57,7.96,-...| (11,[9],[1.0])|          (10,[6],[1.0])|(7,[0],[1.0])|(14,[5],[1.0])|     (9,[0],[1.0])|\n",
      "|  6.0|[1.0,-6.56,5.71,0...| (11,[0],[1.0])|          (10,[1],[1.0])|(7,[4],[1.0])|(14,[1],[1.0])|     (9,[3],[1.0])|\n",
      "|  6.0|[1.0,-1.15,10.16,...| (11,[1],[1.0])|          (10,[1],[1.0])|(7,[4],[1.0])|(14,[2],[1.0])|     (9,[6],[1.0])|\n",
      "|  6.0|[1.0,2.3,8.94,-1....| (11,[4],[1.0])|          (10,[1],[1.0])|(7,[4],[1.0])|(14,[3],[1.0])|     (9,[6],[1.0])|\n",
      "|  6.0|[1.0,-0.29,8.41,-...| (11,[6],[1.0])|          (10,[1],[1.0])|(7,[4],[1.0])|(14,[4],[1.0])|     (9,[6],[1.0])|\n",
      "|  0.0|[1.0,1.36,8.65,0....| (11,[9],[1.0])|          (10,[1],[1.0])|(7,[4],[1.0])|(14,[5],[1.0])|     (9,[6],[1.0])|\n",
      "|  2.0|[0.0,7.81,-6.61,0...| (11,[0],[1.0])|          (10,[2],[1.0])|(7,[0],[1.0])|(14,[1],[1.0])|     (9,[7],[1.0])|\n",
      "|  0.0|[0.0,-4.02,7.13,-...| (11,[2],[1.0])|          (10,[2],[1.0])|(7,[0],[1.0])|(14,[2],[1.0])|     (9,[3],[1.0])|\n",
      "|  0.0|[0.0,-3.36,12.2,0...| (11,[3],[1.0])|          (10,[2],[1.0])|(7,[0],[1.0])|(14,[3],[1.0])|     (9,[0],[1.0])|\n",
      "+-----+--------------------+---------------+------------------------+-------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-6]), r[-2], r[-3], r[-4], r[-5], r[-6]]).\\\n",
    "           toDF(['label','features', 'count_statusH','binned_score_differenceH','base_statusH', 'pitch_numH',\n",
    "                 'latent_pitch_typeH'])\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data= transData(df)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "norm = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n",
    "data = norm.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features_norm=DenseVector([0.0, 0.0, 0.0, -0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0001, 0.0, 0.2178, 0.0, 0.0, 0.7818, 0.0]))]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('features_norm').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['features_norm', 'latent_pitch_typeH','pitch_numH','base_statusH',\n",
    "                                         'binned_score_differenceH','count_statusH'], outputCol = 'features_fin')\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features_fin=SparseVector(67, {0: 0.0, 1: 0.0, 2: 0.0, 3: -0.0, 4: 0.0, 5: 0.0002, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0001, 10: 0.0, 11: 0.2178, 12: 0.0, 14: 0.7818, 16: 1.0, 26: 1.0, 39: 1.0, 50: 1.0, 56: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.select('label', 'features_fin')\n",
    "data.select('features_fin').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features_norm\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = data.filter(data['label'] == 9.0).sample(1165/8285)\\\n",
    ".union(data.filter(data['label'] == 8.0).sample(1165/30932)).union(data.filter(data['label'] == 7.0).sample(1165/50315))\\\n",
    ".union(data.filter(data['label'] == 6.0).sample(1165/109587)).union(data.filter(data['label'] == 5.0).sample(1165/177058))\\\n",
    ".union(data.filter(data['label'] == 4.0).sample(1165/178452)).union(data.filter(data['label'] == 3.0).sample(1165/211755))\\\n",
    ".union(data.filter(data['label'] == 2.0).sample(1165/244465)).union(data.filter(data['label'] == 1.0).sample(1165/330376))\\\n",
    ".union(data.filter(data['label'] == 0.0).sample(1165/763599))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outs',\n",
       " 'pfx_x',\n",
       " 'pfx_z',\n",
       " 'px',\n",
       " 'pz',\n",
       " 'start_speed',\n",
       " 'sz_bot',\n",
       " 'sz_top',\n",
       " 'x0',\n",
       " 'y0',\n",
       " 'z0',\n",
       " 'batter_id',\n",
       " 'inning',\n",
       " 'p_throws',\n",
       " 'pitcher_id',\n",
       " 'stand',\n",
       " 'latent_pitch_typeH',\n",
       " 'pitch_numH',\n",
       " 'base_statusH',\n",
       " 'binned_score_differenceH',\n",
       " 'count_statusH',\n",
       " 'latent_next_pitch']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify layers for the neural network: input layer of size 11 (features), two intermediate of size 5 and 4 and output of size 7 (classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently not working so experimenting with other code\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "layers = [11, 5, 4, 4, 3 , 11]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "FNN = MultilayerPerceptronClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",\\\n",
    "                                         maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "# Chain indexers and forest in a Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, FNN, labelConverter])\n",
    "# train the model\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "layers = [67, 10, 5, 10]\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=1000, featuresCol = 'features_fin',\n",
    "                                         layers=layers, blockSize=128, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o387.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2227.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2227.0 (TID 41098, localhost, executor driver): java.io.IOException: Cannot allocate memory\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:255)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:269)\n\tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)\n\tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)\n\tat java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2663)\n\tat java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2679)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3376)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1962)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:298)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1312)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:612)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:815)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.optimization.LBFGS$CostFun.calculate(LBFGS.scala:261)\n\tat org.apache.spark.mllib.optimization.LBFGS$CostFun.calculate(LBFGS.scala:230)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:41)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:30)\n\tat breeze.optimize.StrongWolfeLineSearch.breeze$optimize$StrongWolfeLineSearch$$phi$1(StrongWolfe.scala:76)\n\tat breeze.optimize.StrongWolfeLineSearch$$anonfun$minimizeWithBound$1.apply$mcVI$sp(StrongWolfe.scala:149)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:148)\n\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:76)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:39)\n\tat breeze.optimize.FirstOrderMinimizer$$anonfun$infiniteIterations$1.apply(FirstOrderMinimizer.scala:64)\n\tat breeze.optimize.FirstOrderMinimizer$$anonfun$infiniteIterations$1.apply(FirstOrderMinimizer.scala:62)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:129)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:71)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:212)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:854)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:249)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot allocate memory\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:255)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:269)\n\tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)\n\tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)\n\tat java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2663)\n\tat java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2679)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3376)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1962)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:298)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1312)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:612)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:815)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cf600386e379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o387.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2227.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2227.0 (TID 41098, localhost, executor driver): java.io.IOException: Cannot allocate memory\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:255)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:269)\n\tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)\n\tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)\n\tat java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2663)\n\tat java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2679)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3376)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1962)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:298)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1312)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:612)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:815)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.optimization.LBFGS$CostFun.calculate(LBFGS.scala:261)\n\tat org.apache.spark.mllib.optimization.LBFGS$CostFun.calculate(LBFGS.scala:230)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:41)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:30)\n\tat breeze.optimize.StrongWolfeLineSearch.breeze$optimize$StrongWolfeLineSearch$$phi$1(StrongWolfe.scala:76)\n\tat breeze.optimize.StrongWolfeLineSearch$$anonfun$minimizeWithBound$1.apply$mcVI$sp(StrongWolfe.scala:149)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:148)\n\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:76)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:39)\n\tat breeze.optimize.FirstOrderMinimizer$$anonfun$infiniteIterations$1.apply(FirstOrderMinimizer.scala:64)\n\tat breeze.optimize.FirstOrderMinimizer$$anonfun$infiniteIterations$1.apply(FirstOrderMinimizer.scala:62)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:129)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:71)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:212)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:854)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:249)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot allocate memory\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:255)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:269)\n\tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)\n\tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)\n\tat java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2663)\n\tat java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2679)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3376)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1962)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:298)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1312)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:612)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:815)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "model = trainer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 38128)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 714, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With layers = [23, 20, 15, 6, 11] accuracy = 0.36312009234584086\n",
    "\n",
    "With layers = [23, 20, 15, 11] accuracy = 0.3631949095780308\n",
    "\n",
    "With layers = [22, 30, 15, 11] accuracy = 0.3613826699538746\n",
    "\n",
    "With layers = [68, 10, 5, 11] accuracy = 0.42275469889813827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40100858600575917"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "evaluator.evaluate(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42275469889813827"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "evaluator.evaluate(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
