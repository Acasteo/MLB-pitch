{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"xor\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and merge dataset on ab_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches = spark.read.option(\"inferSchema\", \"true\").csv('Data/pitches.csv', header = True)\n",
    "atbats = spark.read.option(\"inferSchema\", \"true\").csv('Data/atbats.csv', header = True).select(\"ab_id\", \"batter_id\", \n",
    "                                                                                               \"inning\", \"p_score\", \n",
    "                                                                                               \"p_throws\", \"pitcher_id\",\n",
    "                                                                                               \"stand\", \"top\")\n",
    "\n",
    "df = pitches.join(atbats, \"ab_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ax\", \"ay\", \"az\", \"batter_id\", \"break_angle\", \"break_length\", \"break_y\", \"code\", \"event\", \"g_id\", \"o\",\n",
    "            \n",
    "             \"p_throws\", 'nasty',\"pfx_x\", \"pfx_z\", \"px\", \"pz\", \"spin_dir\", \"end_speed\", \"start_speed\"\n",
    "             \n",
    "             \"sz_bot\", \"sz_top\", \"vx0\", \"vy0\", \"vz0\", \"x\", \"x0\", \"y\", \"y0\", \"z\", \"z0\", \"zone\", \"spin_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variable score_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"score_difference\", df.p_score-df.b_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove low frequency observations (look at pitch_type to decide which ones to remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    (col('pitch_type') != 'UN') &\n",
    "    (col('pitch_type') != 'EP') &\n",
    "    (col('pitch_type') != 'AB') &\n",
    "    (col('pitch_type') != 'FA') &\n",
    "    (col('pitch_type') != 'IN') &\n",
    "    (col('pitch_type') != 'SC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FO and PO are the same so consolidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.na.replace(['FO'], ['PO'], 'pitch_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column that is a latent variable based on pitch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ab_id=2015000044, b_count=0, b_score=0, on_1b=False, on_2b=True, on_3b=False, outs=1, pitch_num=1, pitch_type='FC', s_count=0, start_speed=84.6, sz_bot=1.52, type='B', type_confidence=2.0, inning=5, p_score=3, pitcher_id=425794, stand='L', top=False, score_difference=3, latent_pitch_type=6.0),\n",
       " Row(ab_id=2015000044, b_count=1, b_score=0, on_1b=False, on_2b=True, on_3b=False, outs=1, pitch_num=2, pitch_type='FC', s_count=0, start_speed=88.4, sz_bot=1.52, type='B', type_confidence=2.0, inning=5, p_score=3, pitcher_id=425794, stand='L', top=False, score_difference=3, latent_pitch_type=6.0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"pitch_type\", outputCol=\"latent_pitch_type\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|latent_pitch_type|  count|\n",
      "+-----------------+-------+\n",
      "|              8.0|  43705|\n",
      "|              0.0|1014880|\n",
      "|              7.0|  66484|\n",
      "|              1.0| 450581|\n",
      "|              4.0| 242506|\n",
      "|              3.0| 292789|\n",
      "|              2.0| 337983|\n",
      "|             10.0|   1438|\n",
      "|              6.0| 149756|\n",
      "|              5.0| 234391|\n",
      "|              9.0|  11260|\n",
      "+-----------------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|pitch_type|  count|\n",
      "+----------+-------+\n",
      "|        FT| 337983|\n",
      "|        SL| 450581|\n",
      "|        FC| 149756|\n",
      "|        FF|1014880|\n",
      "|        FS|  43705|\n",
      "|        PO|   1438|\n",
      "|        KC|  66484|\n",
      "|        CH| 292789|\n",
      "|        CU| 234391|\n",
      "|        KN|  11260|\n",
      "|        SI| 242506|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"latent_pitch_type\").count().show()\n",
    "df.groupBy(\"pitch_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## udf_latent_base = udf(lambda z: if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column that is latent variable based on balls and strikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_status(b_count, s_count):\n",
    "    if   b_count==0 and s_count==0: return 0\n",
    "    elif b_count==1 and s_count==0: return 1\n",
    "    elif b_count==0 and s_count==1: return 2\n",
    "    elif b_count==1 and s_count==1: return 3\n",
    "    elif b_count==2 and s_count==0: return 4\n",
    "    elif b_count==0 and s_count==2: return 5\n",
    "    elif b_count==3 and s_count==0: return 6\n",
    "    elif b_count==2 and s_count==1: return 7\n",
    "    elif b_count==1 and s_count==2: return 8\n",
    "    elif b_count==3 and s_count==1: return 9\n",
    "    elif b_count==2 and s_count==2: return 10\n",
    "    elif b_count==3 and s_count==2: return 11\n",
    "    \n",
    "udfcount_status = udf(count_status, IntegerType())\n",
    "df = df.withColumn(\"count_status\", udfcount_status(\"b_count\", \"s_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column that is latent variable based on on_1b, on_2b, and on_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_status(on_1b, on_2b, on_3b):\n",
    "    if   on_1b==0 and on_2b==0 and on_3b==0: return 0\n",
    "    elif on_1b==1 and on_2b==0 and on_3b==0: return 1\n",
    "    elif on_1b==0 and on_2b==1 and on_3b==0: return 2\n",
    "    elif on_1b==0 and on_2b==0 and on_3b==1: return 3\n",
    "    elif on_1b==1 and on_2b==1 and on_3b==0: return 4\n",
    "    elif on_1b==1 and on_2b==0 and on_3b==1: return 5\n",
    "    elif on_1b==0 and on_2b==1 and on_3b==1: return 6\n",
    "    elif on_1b==1 and on_2b==1 and on_3b==1: return 7\n",
    "    \n",
    "udfbase_status = udf(base_status, IntegerType())\n",
    "df = df.withColumn(\"base_status\", udfbase_status(\"on_1b\", \"on_2b\", \"on_3b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column binning score_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_score(score_difference):\n",
    "    if score_difference<(-4): return (-5)\n",
    "    elif score_difference>(4): return (5)\n",
    "    elif score_difference==(-4): return (-4)\n",
    "    elif score_difference==(-3): return (-3) \n",
    "    elif score_difference==(-2): return (-2)\n",
    "    elif score_difference==(-1): return (-1) \n",
    "    elif score_difference==(0): return (0) \n",
    "    elif score_difference==(1): return (1)\n",
    "    elif score_difference==(2): return (2)\n",
    "    elif score_difference==(3): return (3)\n",
    "    elif score_difference==(4): return (4)\n",
    "udfbin_score = udf(bin_score, IntegerType())\n",
    "df = df.withColumn(\"binned_score_difference\", udfbin_score(\"score_difference\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column binning pitch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_pitch_num(pitch_num):\n",
    "    if pitch_num>(14): \n",
    "        return (14)\n",
    "    else: \n",
    "        return(pitch_num)\n",
    "udfpitch_num = udf(bin_pitch_num, IntegerType())\n",
    "df = df.withColumn(\"pitch_num\", udfpitch_num(\"pitch_num\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|pitch_num| count|\n",
      "+---------+------+\n",
      "|       12|   409|\n",
      "|        1|735198|\n",
      "|       13|   150|\n",
      "|        6|145005|\n",
      "|        3|539590|\n",
      "|        5|268666|\n",
      "|        9|  8831|\n",
      "|        4|406352|\n",
      "|        8| 23528|\n",
      "|        7| 60001|\n",
      "|       10|  3210|\n",
      "|       11|  1130|\n",
      "|       14|    80|\n",
      "|        2|653623|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"pitch_num\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to dummy encode some of these because it offers useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('on_1b', 'on_2b', 'on_3b', 'pitch_type', 'type', 'stand', 'top', 'start_speed', 'sz_bot', 'type_confidence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ab_id', 'int'),\n",
       " ('b_count', 'int'),\n",
       " ('b_score', 'int'),\n",
       " ('outs', 'int'),\n",
       " ('pitch_num', 'int'),\n",
       " ('s_count', 'int'),\n",
       " ('inning', 'int'),\n",
       " ('p_score', 'int'),\n",
       " ('pitcher_id', 'int'),\n",
       " ('score_difference', 'int'),\n",
       " ('latent_pitch_type', 'double'),\n",
       " ('count_status', 'int'),\n",
       " ('base_status', 'int'),\n",
       " ('binned_score_difference', 'int')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"ab_id\",\"b_count\",\"b_score\",\"outs\", 'pitch_num', 's_count', 'inning', 'p_score', 'pitcher_id', 'score_difference',\n",
    "              'count_status', 'base_status', 'binned_score_difference', 'latent_pitch_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ab_id', 'int'),\n",
       " ('b_count', 'int'),\n",
       " ('b_score', 'int'),\n",
       " ('outs', 'int'),\n",
       " ('pitch_num', 'int'),\n",
       " ('s_count', 'int'),\n",
       " ('inning', 'int'),\n",
       " ('p_score', 'int'),\n",
       " ('pitcher_id', 'int'),\n",
       " ('score_difference', 'int'),\n",
       " ('count_status', 'int'),\n",
       " ('base_status', 'int'),\n",
       " ('binned_score_difference', 'int'),\n",
       " ('latent_pitch_type', 'double')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 3, -5, 5, 4, -4, -2, 2, -3, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('binned_score_difference').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(subset=[\"count_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+\n",
      "|binned_score_difference| count|\n",
      "+-----------------------+------+\n",
      "|                     -1|340572|\n",
      "|                      1|344147|\n",
      "|                      3|160611|\n",
      "|                     -5|194611|\n",
      "|                      5|203694|\n",
      "|                      4|112138|\n",
      "|                     -4|106880|\n",
      "|                     -2|232629|\n",
      "|                      2|238861|\n",
      "|                     -3|154079|\n",
      "|                      0|757537|\n",
      "+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('binned_score_difference').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  6.0|[2.015000044E9,0....|\n",
      "|  6.0|[2.015000044E9,1....|\n",
      "|  6.0|[2.015000044E9,2....|\n",
      "|  6.0|[2.015000044E9,3....|\n",
      "|  0.0|[2.015000044E9,3....|\n",
      "|  0.0|[2.015000044E9,3....|\n",
      "|  0.0|[2.015000059E9,0....|\n",
      "|  0.0|[2.015000059E9,0....|\n",
      "|  2.0|[2.015000059E9,1....|\n",
      "|  3.0|[2.015000059E9,2....|\n",
      "|  3.0|[2.015000059E9,2....|\n",
      "|  3.0|[2.015000059E9,3....|\n",
      "|  2.0|[2.015000183E9,0....|\n",
      "|  2.0|[2.015000183E9,1....|\n",
      "|  2.0|[2.015000183E9,2....|\n",
      "|  0.0|[2.015000183E9,3....|\n",
      "|  2.0|[2.015000183E9,3....|\n",
      "|  2.0|[2.015000183E9,3....|\n",
      "|  2.0|[2.015000294E9,0....|\n",
      "|  2.0|[2.015000294E9,1....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\\\n",
    "           toDF(['label','features'])\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data= transData(df)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the coefficients and intercept for multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "11 X 13 CSRMatrix\n",
      "\n",
      "Intercept: [2.2653598943433737,1.455193415867617,1.1685980203011852,1.0228151674030306,0.8354762225721429,0.7996999370747043,0.35332721747466383,-0.4636378393412704,-0.8763287240980145,-2.2476738514124417,-4.312829460184991]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the objective per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "1.8945803507804027\n",
      "1.8945803507760104\n"
     ]
    }
   ],
   "source": [
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by label:\n",
      "label 0: 0.3563455070508237\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n",
      "F-measure by label:\n",
      "label 0: 0.5254494598882039\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3563455070508237\n",
      "FPR: 0.3563455070508237\n",
      "TPR: 0.3563455070508237\n",
      "F-measure: 0.18724155421344346\n",
      "Precision: 0.12698212039530862\n",
      "Recall: 0.3563455070508237\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18788721056974578"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.summary of DataFrame[label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSummary = predictions.summary\n",
    "testSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-d9480c827588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfalsePositiveRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightedFalsePositiveRate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtruePositiveRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightedTruePositiveRate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfMeasure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightedFMeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightedPrecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'accuracy'"
     ]
    }
   ],
   "source": [
    "accuracy = testSummary.accuracy\n",
    "falsePositiveRate = testSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = testSummary.weightedTruePositiveRate\n",
    "fMeasure = testSummary.weightedFMeasure()\n",
    "precision = testSummary.weightedPrecision\n",
    "recall = testSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  8.0| 17414|\n",
      "|  0.0|406407|\n",
      "|  7.0| 26761|\n",
      "|  1.0|179941|\n",
      "|  4.0| 96871|\n",
      "|  3.0|117153|\n",
      "|  2.0|134783|\n",
      "| 10.0|   593|\n",
      "|  6.0| 59836|\n",
      "|  5.0| 93880|\n",
      "|  9.0|  4589|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1138228"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
