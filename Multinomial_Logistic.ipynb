{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"xor\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and merge dataset on ab_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").csv('pitches_preprocessed.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('outs', 'pfx_x', 'pfx_z', 'pitch_num', 'px', 'pz', 'start_speed', 'sz_bot', 'sz_top', 'x0', 'y0',\n",
    " 'z0', 'batter_id', 'inning', 'p_throws', 'pitcher_id', 'stand', 'score_difference', 'latent_pitch_type',\n",
    " 'count_status','base_status', 'binned_score_difference','latent_next_pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('binned_score_difference', df.binned_score_difference +5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(inputCols =['outs', 'inning','p_throws', 'stand',\"latent_pitch_type\",\n",
    "                                             \"pitch_num\", \"base_status\",\"binned_score_difference\",\n",
    "                                            \"count_status\"],\n",
    "                                 outputCols =['outsH','inningH','p_throwsH', 'standH', \"latent_pitch_typeH\",\n",
    "                                              \"pitch_numH\", \"base_statusH\",\"binned_score_differenceH\"\n",
    "                                              ,\"count_statusH\"])\n",
    "encoder = encoder.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(pitch):\n",
    "    if  pitch == 0.0: return 1.0/0.36\n",
    "    elif pitch == 1.0: return 1.0/0.17\n",
    "    elif pitch == 2.0: return 1.0/0.12\n",
    "    elif pitch == 3.0: return 1.0/0.1\n",
    "    elif pitch == 4.0: return 1.0/0.08\n",
    "    elif pitch == 5.0: return 1.0/0.08\n",
    "    elif pitch == 6.0: return 1.0/0.05\n",
    "    elif pitch == 7.0: return 1.0/0.02\n",
    "    elif pitch == 8.0: return 1.0/0.01\n",
    "    elif pitch == 9.0: return 1.0/0.01\n",
    "    elif pitch == 10.0: return 1.0/0.01\n",
    "    \n",
    "udfweight = udf(weight, DoubleType())\n",
    "df = df.withColumn(\"weights\", udfweight('latent_next_pitch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pfx_x: double, pfx_z: double, pitch_num: double, px: double, pz: double, start_speed: double, sz_bot: double, sz_top: double, x0: double, y0: double, z0: double, outs: double, batter_id: int, inning: int, p_throws: int, pitcher_id: int, stand: int, score_difference: double, latent_pitch_type: double, count_status: int, base_status: int, binned_score_difference: int, latent_next_pitch: double, weights: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pfx_x','pfx_z','pitch_num','px','pz','start_speed','sz_bot','sz_top','x0','y0','z0','outs',\n",
    "          'batter_id','inning', 'p_throws','pitcher_id','stand', 'score_difference', 'latent_pitch_type',\n",
    "          'count_status', 'base_status','binned_score_difference', 'latent_next_pitch', 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-11], Vectors.dense(r[:10]), r[-1], r[-2], r[-3], r[-4], r[-5], r[-6], r[-7],\n",
    "                                  r[-8], r[-9], r[-10]]).\\\n",
    "           toDF(['label','features', 'standH', 'pitch_numH', 'inningH', 'latent_pitch_typeH',\n",
    "                 'binned_score_differenceH', 'outsH', 'base_statusH', 'count_statusH', 'p_throwsH', 'weights'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['features_norm', 'latent_pitch_typeH', 'standH', 'pitch_numH',\n",
    "                                         'inningH', 'latent_pitch_typeH',\n",
    "                                         'binned_score_differenceH', 'outsH', 'base_statusH', 'count_statusH',\n",
    "                                         'p_throwsH'],\n",
    "                            outputCol = 'features_fin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = assembler.transform(norm.transform(transData(encoder.transform(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features_fin=SparseVector(94, {0: 0.0081, 1: 0.0492, 2: 0.0795, 3: 0.0162, 4: -0.0043, 5: 0.0218, 6: 0.7575, 7: 0.015, 8: 0.031, 9: 0.0175, 10: 1.0, 19: 1.0, 22: 1.0, 36: 1.0, 54: 1.0, 67: 1.0, 74: 1.0, 75: 1.0, 84: 1.0, 93: 1.0}), weights=2.7777777777777777)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.select('label', 'features_fin', 'weights')\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features_fin\", maxIter=100, regParam=0.01,\n",
    "                        elasticNetParam=1.0, family=\"multinomial\", weightCol = 'weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the objective per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.07207912963261243\n",
      "label 1: 0.1228666865073894\n",
      "label 2: 0.08200473941289314\n",
      "label 3: 0.0799067781125613\n",
      "label 4: 0.043635873663262686\n",
      "label 5: 0.07053141180144477\n",
      "label 6: 0.03687442400761443\n",
      "label 7: 0.01746843426134221\n",
      "label 8: 0.2751180590796402\n",
      "label 9: 0.002164586692254908\n"
     ]
    }
   ],
   "source": [
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate by label:\n",
      "label 0: 0.14775679416691478\n",
      "label 1: 0.33026759072095485\n",
      "label 2: 0.23250638580776095\n",
      "label 3: 0.39841133323544015\n",
      "label 4: 0.5451212079507697\n",
      "label 5: 0.23530291697830966\n",
      "label 6: 0.3149396917742812\n",
      "label 7: 0.3002651423849041\n",
      "label 8: 0.6002366540754562\n",
      "label 9: 0.696165191740413\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by label:\n",
      "label 0: 0.5239323334327324\n",
      "label 1: 0.3468558402369556\n",
      "label 2: 0.2662392819937495\n",
      "label 3: 0.38974772901389954\n",
      "label 4: 0.5232747964104064\n",
      "label 5: 0.2239549717149474\n",
      "label 6: 0.32793330273825916\n",
      "label 7: 0.2886079300986298\n",
      "label 8: 0.0372937237736532\n",
      "label 9: 0.5488372093023256\n",
      "Recall by label:\n",
      "label 0: 0.14775679416691478\n",
      "label 1: 0.33026759072095485\n",
      "label 2: 0.23250638580776095\n",
      "label 3: 0.39841133323544015\n",
      "label 4: 0.5451212079507697\n",
      "label 5: 0.23530291697830966\n",
      "label 6: 0.3149396917742812\n",
      "label 7: 0.3002651423849041\n",
      "label 8: 0.6002366540754562\n",
      "label 9: 0.696165191740413\n",
      "F-measure by label:\n",
      "label 0: 0.23050711636515783\n",
      "label 1: 0.33835852503432995\n",
      "label 2: 0.24823206380633955\n",
      "label 3: 0.3940319150775401\n",
      "label 4: 0.5339746460021473\n",
      "label 5: 0.22948874440137917\n",
      "label 6: 0.3213051852184926\n",
      "label 7: 0.2943211543329677\n",
      "label 8: 0.0702242928452579\n",
      "label 9: 0.6137841352405723\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.27751889853742273\n",
      "FPR: 0.08016902170843816\n",
      "TPR: 0.27751889853742273\n",
      "F-measure: 0.2983342102476528\n",
      "Precision: 0.401896255801729\n",
      "Recall: 0.27751889853742273\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27772607183246795"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "predictions = lrModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data accuracy with weights 0.27522829730666976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Reggression using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ['outs', 'pfx_x', 'pfx_z', 'pitch_num', 'px', 'pz', 'start_speed',\n",
    "        'sz_bot', 'sz_top', 'x0', 'y0','z0', 'inning', 'p_throws', 'stand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "                            inputCols=[c for c in df.columns if c in pred],\n",
    "                            outputCol='features').setHandleInvalid('skip')\n",
    "output = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "scaleroutput = scaler.fit(output)\n",
    "scaledoutput = scaleroutput.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=5, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "PC = pca.fit(scaledoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledoutput = PC.transform(scaledoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(outs=1.0, pfx_x=6.08, pfx_z=9.83, pitch_num=2.0, px=-0.532, pz=2.702, start_speed=93.7, sz_bot=1.86, sz_top=3.83, x0=2.161, y0=50.0, z0=6.151, batter_id=120074, inning=1, p_throws=0, pitcher_id=430935, stand=0, score_difference=-1.0, latent_pitch_type=0.0, count_status=2, base_status=0, binned_score_difference=4, latent_next_pitch=0.0, weights=2.7777777777777777, features=DenseVector([1.0, 6.08, 9.83, 2.0, -0.532, 2.702, 93.7, 1.86, 3.83, 2.161, 50.0, 6.151, 1.0, 0.0, 0.0]), scaledFeatures=DenseVector([0.0229, 1.1514, 0.9067, -1.0167, -0.5944, 0.4667, 0.8868, 1.8927, 1.7975, 1.6667, 0.0, 0.7376, -1.4967, -1.6522, -1.1755]), pcaFeatures=DenseVector([-1.7811, -2.8995, -1.4889, -1.8688, -0.383]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledoutput.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"latent_next_pitch\", featuresCol=\"pcaFeatures\", maxIter=100, regParam=0.01,\n",
    "                        elasticNetParam=1.0, family=\"multinomial\", weightCol = 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = scaledoutput.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the objective per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 4.8752663114222614e-05\n",
      "label 1: 0.0019330044085063614\n",
      "label 2: 0.0\n",
      "label 3: 0.04772848384973288\n",
      "label 4: 0.08876871980780623\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.057678256933150956\n",
      "label 8: 0.774735981159234\n",
      "label 9: 0.018773531671517792\n"
     ]
    }
   ],
   "source": [
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate by label:\n",
      "label 0: 6.377827028775389e-05\n",
      "label 1: 0.0023228748090213754\n",
      "label 2: 0.0\n",
      "label 3: 0.053384542198343404\n",
      "label 4: 0.18119833285710063\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0645216792842395\n",
      "label 8: 0.8824643694750135\n",
      "label 9: 0.050031269543464665\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by label:\n",
      "label 0: 0.4117647058823529\n",
      "label 1: 0.19276629570747217\n",
      "label 2: 0.0\n",
      "label 3: 0.1252585272971997\n",
      "label 4: 0.1517527066279377\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.025739937400472243\n",
      "label 8: 0.02000286251737957\n",
      "label 9: 0.010086152553057365\n",
      "Recall by label:\n",
      "label 0: 6.377827028775389e-05\n",
      "label 1: 0.0023228748090213754\n",
      "label 2: 0.0\n",
      "label 3: 0.053384542198343404\n",
      "label 4: 0.18119833285710063\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0645216792842395\n",
      "label 8: 0.8824643694750135\n",
      "label 9: 0.050031269543464665\n",
      "F-measure by label:\n",
      "label 0: 0.0001275367863918249\n",
      "label 1: 0.004590433914315056\n",
      "label 2: 0.0\n",
      "label 3: 0.07486290014028822\n",
      "label 4: 0.16517345907713035\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.03679934055581724\n",
      "label 8: 0.03911901248785403\n",
      "label 9: 0.016787912702853947\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.03828139824960758\n",
      "FPR: 0.027948128742670074\n",
      "TPR: 0.03828139824960758\n",
      "F-measure: 0.02421341585935951\n",
      "Precision: 0.2029140264961137\n",
      "Recall: 0.03828139824960758\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03841162513838457"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "predictions = lrModel.transform(test)\n",
    "predictions = predictions.withColumnRenamed('latent_next_pitch', 'label')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With PCA and Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrpca = LogisticRegression(labelCol=\"label\", featuresCol=\"pcaFeatures\",\n",
    "                        family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(lrpca.regParam, [0.1, 0.01])\\\n",
    ".addGrid(lrpca.elasticNetParam, [0, 1])\\\n",
    ".addGrid(lrpca.maxIter, [1, 5, 10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(\n",
    "    estimator=lrpca,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumnRenamed('latent_next_pitch', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModelpca = crossval.fit(train)\n",
    "lrModelpca = lrModelpca.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumnRenamed('latent_next_pitch', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionspca = lrModelpca.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.350325\n"
     ]
    }
   ],
   "source": [
    "accuracypca = evaluator.evaluate(predictionspca)\n",
    "print(\"Accuracy = %g\" % (accuracypca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision = 0.190237\n"
     ]
    }
   ],
   "source": [
    "precisionpca = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictionspca)\n",
    "print(\"Weighted Precision = %g\" % precisionpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Recall = 0.350325\n"
     ]
    }
   ],
   "source": [
    "recallpca = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictionspca)\n",
    "print(\"Weighted Recall = %g\" % recallpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.183037\n"
     ]
    }
   ],
   "source": [
    "f1pca = MulticlassClassificationEvaluator(metricName=\"f1\").evaluate(predictionspca)\n",
    "print(\"F1 = %g\" % f1pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
